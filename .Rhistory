lines(xseq,est,col='red')
# Kolmogorov-Smirnov Test for equality in distributions
print(ks.test(true,est))
print(head(cbind(P1.x,P2.x,sample)))
plot(xseq,dbeta(xseq,0.5,0.5))
source('~/Dropbox/SharedFolders/ShareTzuo/CardProject/distributionNormalization/nls.R')
source('~/Dropbox/SharedFolders/ShareTzuo/CardProject/distributionNormalization/nls.R')
###############
# Housekeeping
###############
rm(list=ls()) # clear variables
###############
#Simulation
###############
set.seed(20160227)
N <- 1000
P1.x  <-runif(N)
noise <- rnorm(N,mean=1,sd=0.7)
P2.x  <- P1.x + 0.01*noise
P2.x[P2.x<0] <- 0
P2.x[P2.x>1] <- 1
print(head(cbind(P1.x,P2.x)))
print(cor(P1.x,P2.x))
hist(P1.x)
hist(P2.x)
replace <- rbeta(N,3,1)
sample  <- runif(N)
P2.x[sample<0.4] <- sample
print(head(cbind(P1.x,P2.x,sample)))
print(cor(P1.x,P2.x))
hist(P1.x)
hist(P2.x)
###############
# NLS
###############
# nls without  starting values for the parameters even if it throw a warning
nls.1 <- suppressWarnings(nls(P1.x ~ 1-((1-((P2.x)^(1/b)))^(1/a))))
a_start = 1
b_start = 1
# nls with starting values for the parameters
nls.2 <-nls(P1.x ~ 1-((1-((P2.x)^(1/b)))^(1/a)),start=list(a=a_start,b=b_start))
coef <- coef(nls.1)
# Simple goodness of fit
print(summary(nls.1))
print(cor(P1.x,predict(nls.1)))
print(summary(nls.2))
print(cor(P1.x,predict(nls.2)))
xseq = seq(100000)/100000
true = xseq
true = pbeta(xseq)
aest = coef[2]
best = coef[1]
est  = 1-((1-((xseq)^(1/best)))^(1/aest))
###############
# Diagnositics
###############
# Plot Truth vs Estimate
plot(xseq,true,type='l',ylab="CDF",xlab="x-type",main="True vs. Estimated CDF")
lines(xseq,est,col='red')
# Kolmogorov-Smirnov Test for equality in distributions
print(ks.test(true,est))
###############
# Housekeeping
###############
rm(list=ls()) # clear variables
###############
#Simulation
###############
set.seed(20160227)
N <- 1000
P1.x  <-runif(N)
noise <- rnorm(N,mean=1,sd=0.7)
P2.x  <- P1.x + 0.01*noise
P2.x[P2.x<0] <- 0
P2.x[P2.x>1] <- 1
print(head(cbind(P1.x,P2.x)))
print(cor(P1.x,P2.x))
hist(P1.x)
hist(P2.x)
replace <- rbeta(N,3,1)
sample  <- runif(N)
P2.x[sample<0.4] <- sample
print(head(cbind(P1.x,P2.x,sample)))
print(cor(P1.x,P2.x))
hist(P1.x)
hist(P2.x)
###############
# NLS
###############
# nls without  starting values for the parameters even if it throw a warning
nls.1 <- suppressWarnings(nls(P1.x ~ 1-((1-((P2.x)^(1/b)))^(1/a))))
a_start = 1
b_start = 1
# nls with starting values for the parameters
nls.2 <-nls(P1.x ~ 1-((1-((P2.x)^(1/b)))^(1/a)),start=list(a=a_start,b=b_start))
coef <- coef(nls.1)
# Simple goodness of fit
print(summary(nls.1))
print(cor(P1.x,predict(nls.1)))
print(summary(nls.2))
print(cor(P1.x,predict(nls.2)))
xseq = seq(100000)/100000
true = xseq
true = pbeta(xseq,3,1)
aest = coef[2]
best = coef[1]
est  = 1-((1-((xseq)^(1/best)))^(1/aest))
###############
# Diagnositics
###############
# Plot Truth vs Estimate
plot(xseq,true,type='l',ylab="CDF",xlab="x-type",main="True vs. Estimated CDF")
lines(xseq,est,col='red')
# Kolmogorov-Smirnov Test for equality in distributions
print(ks.test(true,est))
source('~/Dropbox/SharedFolders/ShareTzuo/CardProject/distributionNormalization/nls.R')
source('~/Dropbox/SharedFolders/ShareTzuo/CardProject/distributionNormalization/nls.R')
###############
# Housekeeping
###############
rm(list=ls()) # clear variables
###############
#Simulation
###############
set.seed(20160227)
N <- 1000
P1.x  <-runif(N)
noise <- rnorm(N,mean=1,sd=0.7)
P2.x  <- P1.x + 0.01*noise
P2.x[P2.x<0] <- 0
P2.x[P2.x>1] <- 1
print(head(cbind(P1.x,P2.x)))
print(cor(P1.x,P2.x))
hist(P1.x)
hist(P2.x)
replace <- rbeta(N,3,1)
sample  <- runif(N)
P2.x[sample<0.9] <- sample
print(head(cbind(P1.x,P2.x,sample)))
print(cor(P1.x,P2.x))
hist(P1.x)
hist(P2.x)
###############
# NLS
###############
# nls without  starting values for the parameters even if it throw a warning
nls.1 <- suppressWarnings(nls(P1.x ~ 1-((1-((P2.x)^(1/b)))^(1/a))))
a_start = 1
b_start = 1
# nls with starting values for the parameters
nls.2 <-nls(P1.x ~ 1-((1-((P2.x)^(1/b)))^(1/a)),start=list(a=a_start,b=b_start))
coef <- coef(nls.1)
# Simple goodness of fit
print(summary(nls.1))
print(cor(P1.x,predict(nls.1)))
print(summary(nls.2))
print(cor(P1.x,predict(nls.2)))
xseq = seq(100000)/100000
true = xseq
true = pbeta(xseq,3,1)
aest = coef[2]
best = coef[1]
est  = 1-((1-((xseq)^(1/best)))^(1/aest))
###############
# Diagnositics
###############
# Plot Truth vs Estimate
plot(xseq,true,type='l',ylab="CDF",xlab="x-type",main="True vs. Estimated CDF")
lines(xseq,est,col='red')
# Kolmogorov-Smirnov Test for equality in distributions
print(ks.test(true,est))
source('~/Dropbox/SharedFolders/ShareTzuo/CardProject/distributionNormalization/nls.R')
source('~/Dropbox/SharedFolders/ShareTzuo/CardProject/distributionNormalization/nls.R')
source('~/Dropbox/SharedFolders/ShareTzuo/CardProject/distributionNormalization/nls.R')
?
?
?nls
source('~/Dropbox/SharedFolders/ShareTzuo/CardProject/distributionNormalization/nls.R')
?nls
source('~/Dropbox/SharedFolders/ShareTzuo/CardProject/distributionNormalization/nls.R')
source('~/Dropbox/SharedFolders/ShareTzuo/CardProject/distributionNormalization/nls.R')
?nls
source('~/Dropbox/SharedFolders/ShareTzuo/CardProject/distributionNormalization/nls.R')
?nls.control
###############
# Housekeeping
###############
rm(list=ls()) # clear variables
###############
#Simulation
###############
set.seed(20160227)
N <- 1000
P1.x  <-runif(N)
noise <- rnorm(N,mean=1,sd=0.7)
P2.x  <- P1.x + 0.01*noise
P2.x[P2.x<0] <- 0
P2.x[P2.x>1] <- 1
print(head(cbind(P1.x,P2.x)))
print(cor(P1.x,P2.x))
hist(P1.x)
hist(P2.x)
P2.x <- pbeta(P1.x,1,2)
print(head(cbind(P1.x,P2.x)))
print(cor(P1.x,P2.x))
hist(P1.x)
hist(P2.x)
###############
# NLS
###############
nls.control(maxiter = 500, tol = 1e-04, minFactor = 1/1024,
printEval = FALSE, warnOnly = FALSE)
# nls without  starting values for the parameters even if it throw a warning
nls.1 <- suppressWarnings(nls(P1.x ~ 1-((1-((P2.x)^(1/b)))^(1/a))), contr)
a_start = 1
b_start = 1
# nls with starting values for the parameters
nls.2 <-nls(P1.x ~ 1-((1-((P2.x)^(1/b)))^(1/a)),start=list(a=a_start,b=b_start))
coef <- coef(nls.1)
# Simple goodness of fit
print(summary(nls.1))
print(cor(P1.x,predict(nls.1)))
print(summary(nls.2))
print(cor(P1.x,predict(nls.2)))
xseq = seq(100000)/100000
true = xseq
true = pbeta(xseq,3,1)
aest = coef[2]
best = coef[1]
est  = 1-((1-((xseq)^(1/best)))^(1/aest))
###############
# Diagnositics
###############
# Plot Truth vs Estimate
plot(xseq,true,type='l',ylab="CDF",xlab="x-type",main="True vs. Estimated CDF")
lines(xseq,est,col='red')
# Kolmogorov-Smirnov Test for equality in distributions
print(ks.test(true,est))
###############
# Housekeeping
###############
rm(list=ls()) # clear variables
###############
#Simulation
###############
set.seed(20160227)
N <- 1000
P1.x  <-runif(N)
noise <- rnorm(N,mean=1,sd=0.7)
P2.x  <- P1.x + 0.01*noise
P2.x[P2.x<0] <- 0
P2.x[P2.x>1] <- 1
print(head(cbind(P1.x,P2.x)))
print(cor(P1.x,P2.x))
hist(P1.x)
hist(P2.x)
P2.x <- pbeta(P1.x,1,2)
print(head(cbind(P1.x,P2.x)))
print(cor(P1.x,P2.x))
hist(P1.x)
hist(P2.x)
###############
# NLS
###############
nls.control(maxiter = 500, tol = 1e-04, minFactor = 1/1024,
printEval = FALSE, warnOnly = FALSE)
# nls without  starting values for the parameters even if it throw a warning
nls.1 <- suppressWarnings(nls(P1.x ~ 1-((1-((P2.x)^(1/b)))^(1/a))))
a_start = 1
b_start = 1
# nls with starting values for the parameters
nls.2 <-nls(P1.x ~ 1-((1-((P2.x)^(1/b)))^(1/a)),start=list(a=a_start,b=b_start))
coef <- coef(nls.1)
# Simple goodness of fit
print(summary(nls.1))
print(cor(P1.x,predict(nls.1)))
print(summary(nls.2))
print(cor(P1.x,predict(nls.2)))
xseq = seq(100000)/100000
true = xseq
true = pbeta(xseq,3,1)
aest = coef[2]
best = coef[1]
est  = 1-((1-((xseq)^(1/best)))^(1/aest))
###############
# Diagnositics
###############
# Plot Truth vs Estimate
plot(xseq,true,type='l',ylab="CDF",xlab="x-type",main="True vs. Estimated CDF")
lines(xseq,est,col='red')
# Kolmogorov-Smirnov Test for equality in distributions
print(ks.test(true,est))
source('~/Dropbox/SharedFolders/ShareTzuo/CardProject/distributionNormalization/nls.R')
###############
# Housekeeping
###############
rm(list=ls()) # clear variables
###############
#Simulation
###############
set.seed(20160227)
N <- 1000
P1.x  <-runif(N)
noise <- rnorm(N,mean=1,sd=0.7)
P2.x  <- P1.x + 0.01*noise
P2.x[P2.x<0] <- 0
P2.x[P2.x>1] <- 1
print(head(cbind(P1.x,P2.x)))
print(cor(P1.x,P2.x))
hist(P1.x)
hist(P2.x)
P2.x <- pbeta(P1.x,1,2)
print(head(cbind(P1.x,P2.x)))
print(cor(P1.x,P2.x))
hist(P1.x)
hist(P2.x)
###############
# NLS
###############
nls.control(maxiter = 500, tol = 1e-04, minFactor = 1/1024,
printEval = FALSE, warnOnly = FALSE)
# nls without  starting values for the parameters even if it throw a warning
nls.1 <- nls(P1.x ~ 1-((1-((P2.x)^(1/b)))^(1/a)), control = list(maxiter = 500)))
a_start = 1
b_start = 1
# nls with starting values for the parameters
nls.2 <-nls(P1.x ~ 1-((1-((P2.x)^(1/b)))^(1/a)),start=list(a=a_start,b=b_start))
coef <- coef(nls.1)
# Simple goodness of fit
print(summary(nls.1))
print(cor(P1.x,predict(nls.1)))
print(summary(nls.2))
print(cor(P1.x,predict(nls.2)))
xseq = seq(100000)/100000
true = xseq
true = pbeta(xseq,3,1)
aest = coef[2]
best = coef[1]
est  = 1-((1-((xseq)^(1/best)))^(1/aest))
###############
# Diagnositics
###############
# Plot Truth vs Estimate
plot(xseq,true,type='l',ylab="CDF",xlab="x-type",main="True vs. Estimated CDF")
lines(xseq,est,col='red')
# Kolmogorov-Smirnov Test for equality in distributions
print(ks.test(true,est))
nls.1 <- nls(P1.x ~ 1-((1-((P2.x)^(1/b)))^(1/a)), control = list(maxiter = 500))
nls.1 <- nls(P1.x ~ 1-((1-((P2.x)^(1/b)))^(1/a)), control = list(maxiter = 1000))
xseq = seq(100000)/100000
plot(xseq,1-((1-((xseq)^(1/3)))^(1/1)))
xseq = seq(1000)/1000
plot(xseq,1-((1-((xseq)^(1/1)))^(1/3)),type='l')
source('~/Dropbox/SharedFolders/ShareTzuo/CardProject/distributionNormalization/nls.R')
source('~/Dropbox/SharedFolders/ShareTzuo/CardProject/distributionNormalization/nls.R')
source('~/Dropbox/SharedFolders/ShareTzuo/CardProject/distributionNormalization/nls.R')
source('~/Dropbox/SharedFolders/ShareTzuo/CardProject/distributionNormalization/nls.R')
###############
# Housekeeping
###############
rm(list=ls()) # clear variables
###############
#Simulation
###############
set.seed(20160227)
N <- 1000
P1.x  <-runif(N)
noise <- rnorm(N,mean=1,sd=0.7)
P2.x  <- P1.x + 0.01*noise
P2.x[P2.x<0] <- 0
P2.x[P2.x>1] <- 1
print(head(cbind(P1.x,P2.x)))
print(cor(P1.x,P2.x))
hist(P1.x)
hist(P2.x)
#P2.x <- pbeta(P1.x,1,2)
#print(head(cbind(P1.x,P2.x)))
#print(cor(P1.x,P2.x))
#hist(P1.x)
#hist(P2.x)
###############
# NLS
###############
nls.control(maxiter = 500, tol = 1e-04, minFactor = 1/1024,
printEval = FALSE, warnOnly = FALSE)
a_start = 1
b_start = 1
# nls with starting values for the parameters
nls.2 <-nls(P1.x ~ 1-((1-((P2.x)^(1/b)))^(1/a)),start=list(a=a_start,b=b_start),control = list(maxiter = 10000))
coef <- coef(nls.1)
coef <- coef(nls.2)
coef
source('~/Dropbox/SharedFolders/ShareTzuo/CardProject/distributionNormalization/nls.R')
source('~/Dropbox/SharedFolders/ShareTzuo/CardProject/distributionNormalization/nls.R')
source('~/Dropbox/SharedFolders/ShareTzuo/CardProject/distributionNormalization/nls.R')
? import.csv
?? csv
?read.csv
?describe
setwd("~/Dropbox/Programming/Stata_Workshop/Stata_R_Workshop")
###########################################################################
# R Workshop
# University of Pennsylvania
# 11 June 2017
# https://github.com/korykantenga/Stata_R_Workshop
###########################################################################
##################################################
# Housekeeping
##################################################
# Clear Workspace
clear all
# Set Directory (change to your folder)
setwd("~/Dropbox/Programming/Stata_Workshop/Stata_R_Workshop")
############################
# Install and Load Packages
###########################
# Package to describe data
install.packages("Hmisc")
library(Hmisc)
?rm
###########################################################################
# R Workshop
# University of Pennsylvania
# 11 June 2017
# https://github.com/korykantenga/Stata_R_Workshop
###########################################################################
##################################################
# Housekeeping
##################################################
# Clear Workspace
rm(list=ls())
###########################################################################
# R Workshop
# University of Pennsylvania
# 11 June 2017
# https://github.com/korykantenga/Stata_R_Workshop
###########################################################################
##################################################
# Housekeeping
##################################################
# Clear Workspace
rm(list=ls())
# Set Directory (change to your folder)
setwd("~/Dropbox/Programming/Stata_Workshop/Stata_R_Workshop")
############################
# Install and Load Packages
###########################
# Package to describe data
install.packages("Hmisc")
library(Hmisc)
myData <- read.csv("mainData.csv")
describe(myData)
describe(myData)
summary(myData)
?memory
??memory
memory.size()
memory.limit()
memory.size()
memory.limit()
?tabulate
tabulate(info_nil,c(0,1))
tabulate(myData$info_nil,c(0,1))
tabulate(myData$info_nil)
tabulate(myData$info_nil,nbins=2)
table(myData$info_nil)
table(myData$info_nil)
table(myData$info_pos,myData$info_neg)
?table
table(myData$info_nil)/length((myData$info_nil))
10011/(1011+4226)
10011/(10011+4226)
table(myData$info_nil)/length(myData$info_nil)
table(myData$info_pos,myData$info_neg)/length(myData$info_nil)
table(myData$pos_resp,myData$race)
mean(myData$pos_resp[myData$race=="B"])
mean(myData$pos_resp[myData$race=="B"])
mean(myData$pos_resp[myData$race=="W"])
# Since we are working with {0,1} variables, the mean is the % of positive responses
mean(myData$pos_resp[myData$race=="B"])
mean(myData$pos_resp[myData$race=="W"])
# Store the means in "local" variable
# Note: you will see the list of things that are stored in the "Environment Window"
meanB <- mean(myData$pos_resp[myData$race=="B"])
meanW <- mean(myData$pos_resp[myData$race=="W"])
# Calculate the difference
print(meanB-meanW)
### Let's restrict to observations where no information was given
meanB <- mean(myData$pos_resp[myData$race=="B" & myData$info_nil==1])
meanW <- mean(myData$pos_resp[myData$race=="W" & myData$info_nil==1])
print(meanB-meanW)
myData.noinfo <- subset(myData, info_nil==1)
summary(lm(pos_resp ~ black, data = myData.noinfo))
#####################################################
# Regression Analysis
#####################################################
### We can confirm this using a regression!
# Note: The regression with a {0,1} gives the average difference in the outcome
# between the 0 group and the 1 group
# Create a sub datafrarme with variable and data we need
myData.noinfo <- subset(myData, info_nil==1)
# Run regression
summary(lm(pos_resp ~ black, data = myData.noinfo))
### What is the average effect of positive and negative information?
summary(lm(pos_resp ~ info_pos info_neg, data = myData))
lm(pos_resp ~ info_pos info_neg, data = myData)
summary(lm(pos_resp ~ info_pos + info_neg, data = myData))
summary(lm(pos_resp ~ black+info_pos+info_neg+blackxinfo_pos+blackxinfo_neg, data = myData))
View(myData)
### What about giving a black sounding name and different kinds of information?
summary(lm(pos_resp ~ black+info_pos+info_neg+blackXinfo_pos+blackXinfo_neg, data = myData))
-33.7+4.5
